@article{olah2020an,
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  title = {An Overview of Early Vision in InceptionV1},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/circuits/early-vision},
  doi = {10.23915/distill.00024.002}
}
@article{olah2020zoom,
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  title = {Zoom In: An Introduction to Circuits},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/circuits/zoom-in},
  doi = {10.23915/distill.00024.001}
}
@article{olah2018the,
  author = {Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
  title = {The Building Blocks of Interpretability},
  journal = {Distill},
  year = {2018},
  note = {https://distill.pub/2018/building-blocks},
  doi = {10.23915/distill.00010}
}
@article{cammarata2020curve,
  author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Schubert, Ludwig and Petrov, Michael and Olah, Chris},
  title = {Curve Detectors},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/circuits/curve-detectors},
  doi = {10.23915/distill.00024.003}
}
@article{yosinski2015understanding,
  title={Understanding neural networks through deep visualization},
  author={Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
  journal={arXiv preprint arXiv:1506.06579},
  year={2015},
  url={http://yosinski.com/media/papers/Yosinski__2015__ICML_DL__Understanding_Neural_Networks_Through_Deep_Visualization__.pdf}
}
@article{olah2017feature,
  author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
  title = {Feature Visualization},
  journal = {Distill},
  year = {2017},
  url = {https://distill.pub/2017/feature-visualization},
  doi = {10.23915/distill.00007}
}
@article{nguyen2016multifaceted,
  title={Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks},
  author={Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
  journal={arXiv preprint arXiv:1602.03616},
  year={2016},
  url={https://arxiv.org/pdf/1602.03616.pdf}
}
@article{karpathy2015visualizing,
  title={Visualizing and understanding recurrent networks},
  author={Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li},
  journal={arXiv preprint arXiv:1506.02078},
  url={https://arxiv.org/pdf/1506.02078.pdf},
  year={2015}
}
@article{erhan2009visualizing,
  title={Visualizing higher-layer features of a deep network},
  author={Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={University of Montreal},
  volume={1341},
  pages={3},
  year={2009},
  url={https://www.researchgate.net/profile/Aaron_Courville/publication/265022827_Visualizing_Higher-Layer_Features_of_a_Deep_Network/links/53ff82b00cf24c81027da530.pdf}
}
@inproceedings{li2015convergent,
  title={Convergent learning: Do different neural networks learn the same representations?},
  author={Li, Yixuan and Yosinski, Jason and Clune, Jeff and Lipson, Hod and Hopcroft, John E},
  booktitle={FE@ NIPS},
  pages={196--212},
  url={https://arxiv.org/pdf/1511.07543.pdf},
  year={2015}
}
@inproceedings{zeiler2014visualizing,
  title={Visualizing and understanding convolutional networks},
  author={Zeiler, Matthew D and Fergus, Rob},
  booktitle={European conference on computer vision},
  pages={818--833},
  year={2014},
  organization={Springer},
  url={https://arxiv.org/pdf/1311.2901.pdf}
}
@article{vig2019transformervis,
  author    = {Jesse Vig},
  title     = {A Multiscale Visualization of Attention in the Transformer Model},
  journal   = {arXiv preprint arXiv:1906.05714},
  year      = {2019},
  url       = {https://arxiv.org/abs/1906.05714}
}
@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  pages={1097--1105},
  year={2012}
}
@article{simonyan2013deep,
  title={Deep inside convolutional networks: Visualising image classification models and saliency maps},
  author={Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1312.6034},
  year={2013}
}
@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}
@article{zoph2016neural,
  title={Neural architecture search with reinforcement learning},
  author={Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1611.01578},
  year={2016}
}
@article{filan2020neural,
  title={Neural networks are surprisingly modular},
  author={Filan, Daniel and Hod, Shlomi and Wild, Cody and Critch, Andrew and Russell, Stuart},
  journal={arXiv preprint arXiv:2003.04881},
  year={2020}
}@misc{csordas2020neural,
      title={Are Neural Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks},
      author={Róbert Csordás and Sjoerd van Steenkiste and Jürgen Schmidhuber},
      year={2020},
      eprint={2010.02066},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}


@article {Hubel3378,
	author = {Hubel, DH and Livingstone, MS},
	title = {Segregation of form, color, and stereopsis in primate area 18},
	volume = {7},
	number = {11},
	pages = {3378--3415},
	year = {1987},
	doi = {10.1523/JNEUROSCI.07-11-03378.1987},
	publisher = {Society for Neuroscience},
	abstract = {Primate visual cortical area 18 (visual area 2), when stained for the enzyme cytochrome oxidase, shows a pattern of alternating dark and light stripes; in squirrel monkeys, the dark stripes are clearly of 2 alternating types, thick and thin. We have recorded from these 3 subdivisions in macaques and squirrel monkeys, and find that each has distinctive physiological properties: (1) Cells in one set of dark stripes, in squirrel monkeys the thin stripes, are not orientation- selective; a high proportion show color-opponency. (2) Cells in the other set of dark stripes (thick stripes) are orientation-selective; most of them are also selective for binocular disparity, suggesting that they are concerned with stereoscopic depth. (3) Cells in the pale stripes are also orientation-selective and more than half of them are end-stopped. Each of the 3 subdivisions receives a different input from area 17: the thin stripes from the blobs, the pale stripes from the interblobs, the thick stripes from layer 4B. The pale stripes are thus part of the parvocellular system, and the thick stripes part of the magnocellular system. The physiological properties of the cells in the thin and pale stripes reflect the properties of their antecedent cells in 17, but nevertheless exhibit differences that suggest the kinds of processing that might occur at this stage.},
	issn = {0270-6474},
	URL = {https://www.jneurosci.org/content/7/11/3378},
	eprint = {https://www.jneurosci.org/content/7/11/3378.full.pdf},
	journal = {Journal of Neuroscience}
}

@article {Ito3313,
	author = {Ito, Minami and Komatsu, Hidehiko},
	title = {Representation of Angles Embedded within Contour Stimuli in Area V2 of Macaque Monkeys},
	volume = {24},
	number = {13},
	pages = {3313--3324},
	year = {2004},
	doi = {10.1523/JNEUROSCI.4364-03.2004},
	publisher = {Society for Neuroscience},
	abstract = {Angles and junctions embedded within contours are important features to represent the shape of objects. To study the neuronal basis to extract these features, we conducted extracellular recordings while two macaque monkeys performed a fixation task. Angle stimuli were the combination of two straight half-lines larger than the size of the classical receptive fields (CRFs). Each line was drawn from the center to outside the CRFs in 1 of 12 directions, so that the stimuli passed through the CRFs and formed angles at the center of the CRFs. Of 114 neurons recorded from the superficial layer of area V2, 91 neurons showed selective responses to these angle stimuli. Of these, 41 neurons (36.0\%) showed selective responses to wide angles between 60{\textdegree} and 150{\textdegree} that were distinct from responses to straight lines or sharp angles (30{\textdegree}). Responses were highly selective to a particular angle in approximately one-fourth of neurons. When we tested the selectivity of the same neurons to individual half-lines, the preferred direction was more or less consistent with one or two components of the optimal angle stimuli. These results suggest that the selectivity of the neurons depends on both the combination of two components and the responses to individual components. Angle-selective V2 neurons are unlikely to be specific angle detectors, because the magnitude of their responses to the optimal angle was indistinguishable from that to the optimal half-lines. We suggest that the extraction of information of angles embedded within contour stimuli may start in area V2.},
	issn = {0270-6474},
	URL = {https://www.jneurosci.org/content/24/13/3313},
	eprint = {https://www.jneurosci.org/content/24/13/3313.full.pdf},
	journal = {Journal of Neuroscience}
}
