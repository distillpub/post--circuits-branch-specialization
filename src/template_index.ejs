<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style id="distill-article-specific-styles">
    <%=require("../static/styles.css") %>
  </style>
  <script src="https://distill.pub/template.v2.js"></script>
</head>

<body>
  <d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
    </script>
  </d-front-matter>

  <style>
  .header-self-link {
    border-bottom: none;
  }
  .header-self-link:hover {
    border-bottom: none;
  }

  .pixelated {
    image-rendering:optimizeSpeed;             /* Legal fallback */
    image-rendering:-moz-crisp-edges;          /* Firefox        */
    image-rendering:-o-crisp-edges;            /* Opera          */
    image-rendering:-webkit-optimize-contrast; /* Safari         */
    image-rendering:optimize-contrast;         /* CSS3 Proposed  */
    image-rendering:crisp-edges;               /* CSS4 Proposed  */
    image-rendering:pixelated;                 /* CSS4 Proposed  */
    -ms-interpolation-mode:nearest-neighbor;   /* IE8+           */
  }

  .colab-reproduction {
    padding: 2px 4px;
    background: rgba(255, 255, 255, 0.75);
    border-radius: 4px;
    color: #aaa;
    font-weight: 300;
    border: solid 1px rgba(0, 0, 0, 0.08);
    border-bottom-color: rgba(0, 0, 0, 0.15);
    text-transform: uppercase;
    display: inline-block;
    cursor: pointer;
    text-decoration: none;
    /* float: right; */
  }

  .colab-reproduction:hover {
    text-decoration: none;
    border-bottom-color: rgba(0, 0, 0, 0.15);
  }

  .colab-reproduction-first {
    float: left;
    font-size: 9.5pt;
  }

  .colab-reproduction-inline {
    line-height: 100%;
    font-size: 8pt;
  }

  .colab-preface {
    display: inline;
    margin-right: 1em;
  }

  .colab-reproduction-logo {
    transform: translateY(1px);
    height: 10px;
    width: 16px;
  }
  </style>

  <d-title>
    <h1>Branch Specialization</h1>
  </d-title>

  <d-article>

    <h2 style="display: none;">Introduction</h2>

    <p>
      If we think of interpretability as a kind of “anatomy of neural networks,” most of the circuits thread has involved studying tiny little veins &ndash; looking at the small-scale, at individual neurons and how they connect. However, there are many natural questions that the small-scale approach doesn't address.
    </p>

    <p>
      In contrast, the most prominent abstractions in biological anatomy involve larger-scale structures: individual organs like the heart, or entire organ systems like the respiratory system. And so we wonder: is there a “respiratory system” or “heart” or “brain region” of an artificial neural network? Do neural networks have any emergent structures that we could study that are larger-scale than circuits?
    </p>

    <p>
      This article describes <i>branch specialization</i>, one of three larger “structural phenomena” we’ve been able observe in neural networks. (The other two, <a href="https://distill.pub/2020/circuits/equivariance/">equivariance</a> and <a>weight banding</a>, have separate dedicated articles. Branch specialization occurs when neural network layers are split up into branches. The neurons and circuits tend to self-organize, clumping related functions into each branch and forming larger functional units &ndash; a kind of “neural network brain region.” We find evidence that these structures implicitly exist in neural networks without branches, and that branches are simply reifying structures that otherwise exist.
    </p>

    <p>
      The earliest example of branch specialization that we're aware of comes from AlexNet<d-cite bibtex-key="krizhevsky2012imagenet"></d-cite>. AlexNet is famous as a jump in computer vision, arguably starting the deep learning revolution, but buried in the paper is a fascinating, rarely-discussed detail.

      The first two layers of AlexNet are split into two branches which can't communicate until they rejoin after the second layer. This structure was used to maximize the efficiency of training the model on two GPUs, but the authors noticed something very curious happened as a result. The neurons in the first layer organized themselves into two groups: black-and-white Gabor filters formed on one branch and low-frequency color detectors formed on the other branch.
    </p>

    <figure id="figure-1">
      <img src="images/Figure_1.png" style="max-width: 100%; width: auto;" />
      <figcaption class="figcaption l-body">
        <p>
          <a href="#figure-1" class="figure-number">1</a>. Branch specialization in the first two layers of AlexNet. Krizhevsky et al.<d-cite bibtex-key="krizhevsky2012imagenet"></d-cite> observed the phenomenon we call branch specialization in the first layer of AlexNet by visualizing their weights to RGB channels; here, we use <a href="https://distill.pub/2017/feature-visualization/">feature visualization</a><d-cite bibtex-key="erhan2009visualizing,simonyan2013deep,nguyen2016multifaceted,olah2017feature"></d-cite> to show how this phenomenon extends to the second layer of each branch.
        </p>
      </figcaption>
    </figure>

    <p>
      Although the first layer of AlexNet is the only example of branch specialization we're aware of in the literature, it seems to be a common phenomenon. We find that branch specialization happens in later hidden layers, not just the first layer. It occurs in both low-level and high-level features. It occurs in a wide range of models, including places you might not expect it &ndash; for example, residual blocks in resnets can functionally be branches and specialize. Finally, branch specialization appears to surface as a structural phenomenon in plain convolutional nets, even without any particular structure causing it.
    </p>

    <p>
      Is there a large-scale structure to how neural networks operate? How are features and circuits organized within the model? Does network architecture influence the features and circuits that form? Branch specialization hints at an exciting story related to all of these questions.
    </p>

    <h2>What is a branch?</h2>

    <p>
      Many neural network architectures have <i>branches</i>, sequences of layers which temporarily don't have access to "parallel" information which is still passed to later layers.
    </p>

    <figure id="figure-2">
      {diagrams/Figure_2}
      <figcaption class="figcaption l-body">
        <p>
          <a href="#figure-2" class="figure-number">2</a>. Examples of branches in various types of neural network architectures.
        </p>
      </figcaption>
    </figure>

    <p>
      In the past, models with explicitly-labeled branches were popular (such as AlexNet and the Inception family of networks<d-cite bibtex-key="szegedy2015going"></d-cite>). In more recent years, these have become less common, but residual networks &ndash; which can be seen as implicitly having branches in their residual blocks &ndash; have become very common. We also sometimes see branched architectures develop automatically in neural architecture search<d-cite bibtex-key="zoph2016neural"></d-cite>, an approach where the network architecture is learned.
    </p>

    <p>
      The implicit branching of residual networks has some important nuances. At first glance, every layer is a two-way branch. But because the branches are combined together by addition, we can actually rewrite the model to reveal that the residual blocks can be understood as branches in parallel:
    </p>

    <figure id="figure-3">
      {diagrams/Figure_3}
      <figcaption class="figcaption l-body">
        <p>
          <a href="#figure-3" class="figure-number">3</a>. Residual blocks as branches in parallel.
        </p>
      </figcaption>
    </figure>

    <p>
      We typically see residual blocks specialize in very deep residual networks (e.g. ResNet-152<!--CITE-->). One hypothesis for why is that, in these models, the exact depth of a layer doesn't matter and the branching aspect becomes more important than the sequential aspect.
    </p>

    <p>
      One of the conceptual weaknesses of normal branching models is that although branches can save parameters, it still requires a lot of parameters to mix values between branches. However, if you buy the branch interpretation of residual networks, you can see them as a strategy to sidestep this: residual networks intermix branches (e.g. block sparse weights) with low-rank connections (projecting all the blocks into the same sum and then back up). This seems like a really elegant way to handle branching. More practically, it suggests that analysis of residual networks might be well-served by paying close attention to the units in the blocks, and that we might expect the residual stream to be unusually polysemantic.
    </p>

    <h2>Why does branch specialization occur?</h2>

    <p>
      Branch specialization is defined by features organizing between branches. In a normal layer, features are organized randomly: a given feature is just as likely to be any neuron in a layer. But in a branched layer, we often see features of a given type cluster to one branch. The branch has specialized on that type of feature.
    </p>

    <p>
      How does this happen? Our intuition is that there's a positive feedback loop during training.
    </p>

    <figure id="figure-4">
      {diagrams/Figure_4}
      <figcaption class="figcaption l-body">
        <p>
          <a href="#figure-4" class="figure-number">4</a>. Hypothetical positive feedback loop of branch specialization during training.
        </p>
      </figcaption>
    </figure>

    <p>
      Another way to think about this is that if you need to cut a neural network into pieces that have limited ability to communicate with each other, it makes sense to organize similar features close together, because they probably need to share more information.
    </p>

    <h2>Branch specialization beyond the first layer</h2>

    <p>
      So far, the only concrete example we've shown of branch specialization is the first and second layer of AlexNet. What about later layers? AlexNet also splits its later layers into branches, after all. This seems to be unexplored, since studying features after the first layer is much harder.<d-footnote>For the first layer, one can visualize the RGB weights; for later layers, one needs to use feature visualization.</d-footnote>
    </p>

    <p>
      Unfortunately, branch specialization in the later layers of AlexNet is also very subtle. Instead of one overall split, it's more like there's dozens of small clusters of neurons, each cluster being assigned to a branch. It's hard to be confident that one isn't just seeing patterns in noise.
    </p>

    <p>
      But other models have very clear branch specialization in later layers. This tends to happen when a branch constitutes only a very small fraction of a layer, either because there are many branches or because one is much smaller than others. In these cases, the branch can specialize on a very small subset of the features that exist in a layer and reveal a clear pattern.
    </p>

    <p>
      For example, most of InceptionV1's layers have a branched structure. The branches have varying numbers of units, and varying convolution sizes. The 5x5 branch is the smallest branch, and also has the largest convolution size. It's often very specialized:
    </p>


    <figure id="figure-5" style="grid-column-start: page-start; grid-column-end: page-end;">
      {diagrams/Figure_5}
      <figcaption class="figcaption l-body">
        <p>
          <a href="#figure-5" class="figure-number">5</a>. Examples of branch specialization in <code><a href="https://distill.pub/2017/feature-visualization/appendix/googlenet/3a.html#3a-192">mixed3a_5x5</a></code>, <code><a href="https://distill.pub/2017/feature-visualization/appendix/googlenet/3b.html#3b-320">mixed3b_5x5</a></code>, and <code><a href="https://distill.pub/2017/feature-visualization/appendix/googlenet/4a.html#4a-396">mixed4a_5x5</a></code>.
        </p>
      </figcaption>
    </figure>

    <p>
      This is exceptionally unlikely to have occurred by chance.

      For example, all 9 of the black and white vs. color detectors in <code>mixed3a</code> are in <code>mixed3a_5x5</code>, despite it only being 32 out of the 256 neurons in the layer. The probability of that happening by chance is less than 1/10<sup>8</sup>. For a more extreme example, all 30 of the curve-related features in <code>mixed3b</code> are in <code>mixed3b_5x5</code>, despite it being only 96 out of the 480 neurons in the layer. The probability of that happening by chance is less than 1/10<sup>20</sup>.


    <p>
      It's worth noting one confounding factor which might be influencing the specialization. The 5x5 branches are the smallest branches, but also have larger convolutions (5x5 instead of 3x3 or 1x1) than their neighbors.<d-footnote>There is something which suggests that the branching plays an essential role: mixed3a and mixed3b are adjacent layers which contain relatively similar features and are at the same scale. If it was only about convolution size, why don't we see any curves in the <code>mixed3a_5x5</code> branch or color in the <code>mixed3b_5x5</code> branch?</d-footnote>
    </p>

    <h2>Why is branch specialization consistent?</h2>

    <p>
      Perhaps the most surprising thing about branch specialization is that the same branch specializations seem to occur again and again, across different architectures and tasks.
    </p>

    <p>
      For example, the branch specialization we observed in AlexNet &ndash; the first layer specializing into a black-and-white Gabor branch vs. a low-frequency color branch &ndash; is a surprisingly robust phenomenon. It occurs consistently if you retrain AlexNet. It also occurs if you train other architectures with the first few layers split into two branches. It even occurs if you train those models on other natural image datasets, like Places instead of ImageNet. Anecdotally, we also seem to see other types of branch specialization recur. For example, finding branches that seem to specialize in curve detection seems to be quite common (although InceptionV1's <code>mixed3b_5x5</code> is the only one we've carefully characterized).
    </p>

    <p>
      So, why do the same branch specializations occur again and again?
    </p>

    <p>
      One hypothesis seems very tempting. Notice that many of the same features that form in normal, non-branched models also seem to form in branched models. For example, the first layer of both branched and non-branched models contain Gabor filters and color features. If the same features exist, presumably the same weights exist between them.
    </p>



    <p>
      Could it be that branching is just surfacing a structure that already exists? Perhaps there's two different subgraphs between the weights of the first and second conv layer in a normal model, with relatively small weights between them, and when you train a branched model these two subgraphs latch onto the branches.

      (This would be directionally similar to work finding modular substructures<d-cite bibtex-key="filan2020neural,csordas2020neural"></d-cite> within neural networks.)
    </p>

    <p>
      To test this, let's look at models which have non-branched first and second convolutional layers. Let's take the weights between them and perform a singular value decomposition (SVD) on the absolute values of the weights. This will show us the main factors of variation in which neurons connect to different neurons in the next layer (irrespective of whether those connections are excitatory or inhibitory).
    </p>

    <p>
      Sure enough, the singular vector (the largest factor of variation) of the weights between the first two convolutional layers of InceptionV1 is color.
    </p>

    <figure id="figure-6"  style="grid-column-start: page-start;">
      {diagrams/Figure_6}
      <figcaption class="figcaption l-body">
        <p>
          <a href="#figure-6" class="figure-number">6</a>. Singular vectors for the first and second convolutional layers of InceptionV1, trained on ImageNet (above) or Places365 (below). One can think of neurons being plotted closer together in this diagram as meaning they likely tend to connect to similar neurons.

        </p>
      </figcaption>
    </figure>

    <p>
      We also see that the second factor appears to be frequency. This suggests an interesting prediction: perhaps if we were to split the layer into more than two branches, we'd also observe specialization in frequency in addition to color.
    </p>

    <p>
      This seems like it may be true. For example, here we see a high-frequency black-and-white branch, a mid-frequency mostly black-and-white branch, a mid-frequency color branch, and a low-frequency color branch.
    </p>

    <figure id="figure-7">
      <img src="images/Figure_7.png" style="max-width: 100%; width: auto;" />
      <figcaption class="figcaption l-body">
        <p>
          <a href="#figure-7" class="figure-number">7</a>. We constructed a small ImageNet model with the first layer split into four branches. The rest of the model is roughly an InceptionV1 architecture.
        </p>
      </figcaption>
    </figure>

    <h2>Parallels to neuroscience</h2>


    <p>
      We've shown that branch specialization is one example of a structural phenomenon &mdash; a larger-scale structure in a neural network. It happens in a variety of situations and neural network architectures, and it happens with <i>consistency</i> &ndash; certain motifs of specialization, such as color, frequency, and curves, happen consistently across different architectures and tasks.
    </p>

    <p>
      Harkening back to our comparison with anatomy, although we hesitate to claim explicit parallels to neuroscience, it's tempting to draw analogies between branch specialization and the existence of regions of the brain focused on particular tasks. The visual cortex, the auditory cortex, Broca's area and Wernicke's area  – these are all examples of brain areas with such consistent specialization across wide populations of people that neuroscientists and psychologists have been able to characterize as having remarkably consistent functions.

      As researchers without expertise in neuroscience, we're uncertain how useful this connection is, but it may be worth considering whether branch specialization can be a useful model of how specialization might emerge in biological neural networks.
    </p>
  </d-article>

  <d-appendix>
    <h3>Author Contributions</h3>

    <p>As with many scientific collaborations, the contributions are difficult to separate because it was a collaborative effort that we wrote together.</p>

    <p><b>Research.</b> The phenomenon of branch specialization was initially observed by Chris Olah. Chris also developed the weight PCA experiments suggesting that it implicitly occurs in non-branched models. This investigation was done in the context of and informed by collaborative research into circuits by Nick Cammarata, Gabe Goh, Chelsea Voss, Ludwig Schubert, and Chris. Chelsea and Nick contributed to framing this work in the importance of larger scale structures on top of circuits.</p>

    <p><b>Infrastructure.</b> Branch specialization was only discovered because an early version of Microscope by Ludwig Schubert made it easy to browse the neurons that exist at certain layers. Michael Petrov, Ludwig and Nick built a variety of infrastructural tools which made our research possible.</p>

    <p><b>Writing and Diagrams.</b> An initial article was written and illustrated by Chris. It was revised and improved by Chelsea.</p>

    <h3>Acknowledgments</h3>

    <p>
      We are grateful to participants of #circuits in the Distill Slack for their engagement on this article, and especially to Alex Bäuerle, Ben Egan, Patrick Mineault, Matt Nolan, and Vincent Tjeng for their remarks on a first draft.
    </p>

    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <d-bibliography src="bibliography.bib"></d-bibliography>

</body>
