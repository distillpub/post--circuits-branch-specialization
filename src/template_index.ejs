<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style id="distill-article-specific-styles">
    <%=require("../static/styles.css") %>
  </style>
  <script src="https://distill.pub/template.v2.js"></script>
</head>

<body>
  <d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
    </script>
  </d-front-matter>

  <style>
  .header-self-link {
    border-bottom: none;
  }
  .header-self-link:hover {
    border-bottom: none;
  }

  .pixelated {
    image-rendering:optimizeSpeed;             /* Legal fallback */
    image-rendering:-moz-crisp-edges;          /* Firefox        */
    image-rendering:-o-crisp-edges;            /* Opera          */
    image-rendering:-webkit-optimize-contrast; /* Safari         */
    image-rendering:optimize-contrast;         /* CSS3 Proposed  */
    image-rendering:crisp-edges;               /* CSS4 Proposed  */
    image-rendering:pixelated;                 /* CSS4 Proposed  */
    -ms-interpolation-mode:nearest-neighbor;   /* IE8+           */
  }

  .colab-reproduction {
    padding: 2px 4px;
    background: rgba(255, 255, 255, 0.75);
    border-radius: 4px;
    color: #aaa;
    font-weight: 300;
    border: solid 1px rgba(0, 0, 0, 0.08);
    border-bottom-color: rgba(0, 0, 0, 0.15);
    text-transform: uppercase;
    display: inline-block;
    cursor: pointer;
    text-decoration: none;
    /* float: right; */
  }

  .colab-reproduction:hover {
    text-decoration: none;
    border-bottom-color: rgba(0, 0, 0, 0.15);
  }

  .colab-reproduction-first {
    float: left;
    font-size: 9.5pt;
  }

  .colab-reproduction-inline {
    line-height: 100%;
    font-size: 8pt;
  }

  .colab-preface {
    display: inline;
    margin-right: 1em;
  }

  .colab-reproduction-logo {
    transform: translateY(1px);
    height: 10px;
    width: 16px;
  }
  </style>

  <d-title>
    <h1>Branch Specialization</h1>
  </d-title>

  <d-article>

    <h2 style="display: none;">Introduction</h2>

    <p>
      If we think of interpretability as a kind of “anatomy of neural networks”, most of the circuits thread has involved studying tiny little veins &ndash; looking at the small-scale, at individual neurons and how they connect. However, there are many natural questions that the small-scale approach doesn't address.
    </p>

    <p>
      In contrast, biological anatomy also has abstractions for dealing with larger-scale structures: individual organs like the heart, or entire organ systems like the respiratory system. And so we wonder: is there a “respiratory system” or “heart” or “brain region” of an artificial neural network? Do neural networks have any emergent structures that we could study that are larger-scale than circuits?
    </p>

    <p>
      This article describes <i>branch specialization</i>, one of three larger “structural phenomena” we’ve been able observe in neural networks. (The other two, equivariance and weight banding, have separate dedicated articles. Branch specialization occurs when neural network layers are split up into branches. The neurons and circuits tend to self-organize, clumping related functions into the each branch and forming larger functional units &ndash; a kind of “neural network brain region.” We find evidence that these structures implicitly exist in neural networks without branches, and that branches are simply reifying structures that otherwise exist.
    </p>

    <p>
      AlexNet[] is famous as a jump in computer vision, arguably starting the deep learning revolution. But buried in the details of the paper is a fascinating, rarely-discussed detail.
    </p>

    <p>
      The first two layers of AlexNet are split into two branches which can't communicate until they rejoin after the second layer. This structure was used to maximize the efficiency of training the model on two GPUs, but authors noticed something very curious happened as a result. The neurons in the first layer organized themselves into two groups: black-and-white Gabor filters formed on one branch and low-frequency color detectors formed on the other branch.
    </p>

    <figure id="figure-1">
      <img src="images/Figure_1.png" style="max-width: 100%; width: auto;" />
      <figcaption class="figcaption l-body">
        <p>
          <a href="#figure-1" class="figure-number">1</a>. CAPTION.
        </p>
      </figcaption>
    </figure>

    <p>
      Although the first layer of AlexNet is the only example of branch specialization we're aware of in the literature, it seems to be quite common. We find that branch specialization happens in later hidden layers, not just the first layer. It occurs in both low-level and high-level features. It occurs in a wide range of models, including places you might not expect it &ndash; for example, residual blocks in resnets can functionally be branches and specialize. Finally, branch specialization appears to surface as a structural phenomenon even in vanilla convolutional nets, even without any particular structure causing it.
    </p>

    <p>
      Is there a large-scale structure to how neural networks operate? How are features and circuits organized within the model? Does network architecture influence the features and circuits that form? Branch specialization hints at an exciting story related to all of these questions.
    </p>

    <h2>What is a branch?</h2>

    <p>
      Many neural network architectures have <i>branches</i>, sequences of layers which temporarily don't have access to information which is still passed to later layers.
    </p>

    <figure id="figure-2">
      {diagrams/Figure_2}
      <figcaption class="figcaption l-body">
        <p>
          <a href="#figure-2" class="figure-number">2</a>. CAPTION.
        </p>
      </figcaption>
    </figure>

    <p>
      In the past, models with explicitly-labeled branches were popular (such as AlexNet and the Inception family of networks). In more recent years, these have become less common, but residual networks &ndash; which can be seen as implicitly having branches in their residual blocks &ndash; have become very common. We also sometimes see branched architectures develop automatically in AutoML approaches, where the network architecture is learned.
    </p>

    <p>
      The implicit branching of residual networks has some important nuances. At first glance, every layer is a two-way branch. But because the branches are combined together by addition, we can actually rewrite the model to reveal that the residual blocks can be understood as branches in parallel:
    </p>

    <figure id="figure-3">
      {diagrams/Figure_3}
      <figcaption class="figcaption l-body">
        <p>
          <a href="#figure-3" class="figure-number">3</a>. CAPTION.
        </p>
      </figcaption>
    </figure>

    <p>
      We typically see residual blocks specialize in very deep residual networks (e.g. ResNet-52). One hypothesis for why is that, in these models, the exact depth of a layer doesn't matter and the branching aspect becomes more important than the sequential aspect.
    </p>

    <p>
      One of the conceptual weaknesses of normal branching models is that although branches can save parameters, it still requires a lot of parameters to mix values between branches. However, if you buy the branch interpretation of residual networks, you can see them as a strategy to sidestep this: residual networks intermix branches (e.g. block sparse weights) with low-rank connections (projecting all the blocks into the same sum and then back up). This seems like a really elegant way to handle branching. More practically, it suggests that analysis of residual networks might be well-served by paying close attention to the units in the blocks, and might expect the residual stream to be unusally polysemantic.
    </p>

    <h2>Why does branch specialization occur?</h2>

    <p>
      Branch specialization is defined by features organizing between branches. In a normal layer, features are organized randomly: a given feature is just as likely to be any neuron in a layer. But in a branched layer, we often see features of a given type cluster to one branch. The branch has specialized on that type of feature.
    </p>

    <p>
      How does this happen? Our intuition is that there's a positive feedback loop during training.
    </p>

    <figure id="figure-4">
      {diagrams/Figure_4}
      <figcaption class="figcaption l-body">
        <p>
          <a href="#figure-4" class="figure-number">4</a>. CAPTION.
        </p>
      </figcaption>
    </figure>

    <p>
      Another way to think about this is that if you need to cut a neural network into pieces that have limited ability to communicate with each other, it makes sense to organize similar features close together, because they probably need to share more information.
    </p>

    <h2>Branch specialization beyond the first layer</h2>

    <p>
      So far, the only concrete example we've shown of branch specialization is the first and second layer of AlexNet. What about later layers? AlexNet also splits its later layers into branches, after all. This seems to be unexplored, since studying features after the first few layers is much harder.
    </p>

    <p>
      Unfortunately, branch specialization in the later layers of AlexNet is very subtle. Instead of one overall split, it's more like there's dozens of small clusters of neurons, each cluster being assigned to a branch. It's hard to be confident that one isn't just seeing patterns in noise.
    </p>

    <p>
      But other models have very clear branch specialization in later layers. This tends to happen when a branch constitutes only a very small fraction of a layer, either because there are many branches or because one is much smaller than others. In these cases, the branch can specialize on a very small subset of the features that exist in a layer and reveal a clear pattern.
    </p>

    <p>
      For example, most of InceptionV1's layers have a branched structure. The branches have varying numbers of units, and varying convolution sizes. The 5x5 branch is the smallest branch, and also has the largest convolution size. It's often very specialized:
    </p>

    <ul>
      <li>
        <code>mixed3a_5x5</code>:
        The 5x5 branch of mixed3a, a relatively early layer, is specialized on color detection. For example, all 9 of the black-and-white vs. color detectors are in this branch, despite it only being 32 of this layer's 256 neurons. The probability of that happening by chance is less than 1/10<sup>8</sup>.
      </li>
      <li>
        <code>mixed3b_5x5</code>:
        This branch contains all 30 of the curve-related features for this layer (all curves, double curves, circles, spirals, S-shaped features, and more), despite only being 96 of the 480 neurons in this layer. The probability of that happening by chance is less than 1/10<sup>20</sup>. Many of the other neurons in this branch are boundary detectors, which often partly rely on curves if they are detecting curve boundaries.
      </li>
      <li>
        <code>mixed4a_5x5</code>:
        This branch appears to be specialized in complex shapes and 3D geometry detectors. We don't have enough of a full taxonomy of this layer to allow for a quantitative assessment.
      </li>
    </ul>

    <p>
      It's worth noting one confounding factor which might be influencing the specialization. The 5x5 branches are the smallest branches, but also have larger convolutions (5x5 instead of 3x3 or 1x1) than their neighbors.<d-footnote>There is something which suggests that the branching plays an essential role: mixed3a and mixed3b are adjacent layers which contain relatively similar features and are at the same scale. If it was only about convolution size, why don't we see any curves in the <code>mixed3a_5x5</code> branch or color in the <code>mixed3b_5x5</code> branch?</d-footnote>
    </p>

    <h2>Why is branch specialization consistent?</h2>

    <p>
      Perhaps the most surprising thing about branch specialization is that the same branch specializations seem to occur again and again, across different architectures and tasks.
    </p>

    <p>
      For example, the branch specialization we observed in AlexNet &ndash; the first layer specializing into a black-and-white Gabor branch vs. a low-frequency color branch &ndash; is a surprisingly robust phenomenon. It occurs consistently if you retrain AlexNet. It also occurs if you train other architectures with the first few layers split into two branches. It even occurs if you train those models on other natural image datasets, like Places instead of ImageNet. Anecdotally, we also seem to see other types of branch specialization recur. For example, finding branches that seem to specialize in curve detection seems to be quite common (although InceptionV1's <code>mixed3b_5x5</code> is the only one we've carefully characterized).
    </p>

    <p>
      So, why do the same branch specializations occur again and again?
    </p>

    <p>
      One hypothesis seems very tempting. Notice that many of the same features seem to form in branched models as form in normal, non-branched models. For example, the first layer of both branched and non-branched models contain Gabor filters and color features. If the same features exist, presumably the same weights exist between them.
    </p>

    <p>
      Could it be that branching is just surfacing a structure that already exists? Perhaps there's two different subgraphs between the weights of the first and second conv layer in a normal model, with relatively small weights between them, and that when you train a branched model these two subgraphs latch onto the branches.
    </p>

    <p>
      To test this, let's look at models which have non-branched first and second convolutional layers. Let's take the weights between them and perform a singular value decomposition (SVD) on the absolute values of the weights. This will show us the main factors of variation in which neurons connect to different neurons in the next layer (irrespective of whether those connections are excitatory or inhibitory).
    </p>

    <p>
      Sure enough, the singular vector (the largest factor of variation) of the first two convolutional layers of InceptionV1 is color.
    </p>

    <figure id="figure-5">
      {diagrams/Figure_5}
      <figcaption class="figcaption l-body">
        <p>
          <a href="#figure-5" class="figure-number">5</a>. CAPTION.
        </p>
      </figcaption>
    </figure>

    <p>
      We also see that the second factor appears to be frequency. This suggests an interesting prediction: perhaps if we were to split the layer into more than two branches, we'd also observe spcialization in frequency in addition to color.
    </p>

    <p>
      This seems like it may be true. For example, here we see a high-frequency black-and-white branch, a mid-frequency mostly black-and-white branch, a mid-frequency color branch, and a low-frequency color branch.
    </p>

    <figure id="figure-6">
      <img src="images/Figure_6.png" style="max-width: 100%; width: auto;" />
      <figcaption class="figcaption l-body">
        <p>
          <a href="#figure-6" class="figure-number">6</a>. CAPTION.
        </p>
      </figcaption>
    </figure>

    <h2>Parallels to neuroscience</h2>


    <p>
      We've shown that branch specialization is one example of a large-scale structure to how neural networks operate, an example of a structural phenomenon. It happens in a variety of situations and neural network architectures, and it happens with <i>consistency</i> &ndash; certain motifs of specialization, such as color, frequency, and curves, happen consistently across different architectures and tasks.
    </p>

    <p>
      Harkening back to our comparison with anatomy, although we hesitate to claim explicit parallels to neuroscience, it's tempting to draw analogies between branch specialization and the existence of regions of the brain focused on particular tasks. The visual cortex, the auditory cortex, the anatomical homunculus[], Broca's area and Wernicke's area  – these are all examples of brain areas with such consistent specialization across wide populations of people that neuroscientists and psychologists have been able to characterize their function as well as to notice remarkably consistent symptoms, such as those of Wernicke's aphasia, when those areas are damaged.
    </p>
  </d-article>

  <d-appendix>
    <h3>Author Contributions</h3>

    <!-- <p>This article grew out of a document that Chris wrote in order to act as an explanation for our techniques for visualizing weights.</p>

    <p>
      <b>Research.</b> The necessity of visualizing weights is a problem we encounter frequently, and our techniques have been refined across many investigations of features and circuits, so it is difficult to fully separate out all contributions towards improving those techniques.
    </p>

    <p>
      Many people "test drove" these visualization methods, and a lot of our practical knowledge of using them to study circuits came from that.

      For example, the curve detector examples used in <i>Small Multiples</i> are due to Nick's work investigating curve detectors. Gabe performed experiments that moved <i>Visualizing Spatial Position Weights</i> forward. The high-low frequency detector example and NMF factors used in <i>Visualizing Weight Factors</i> are due to experiments performed by Ludwig, and the weight banding examples in <i>Aside: One Simple Trick</i> are due to experiments run by Michael.
    </p>

    <p>
      <b>Writing and Diagrams.</b> Chris wrote the article and developed the designs for its original figures. Chelsea ported the article to Distill, upgraded the diagrams for the new format, edited some text, and developed Figure 12, and Chris provided feedback on the ported article.
    </p> -->

    <h3>Acknowledgments</h3>

    <!-- <p>
      We are grateful to participants of #circuits in the Distill slack for their pre-draft comments and engagement with these concepts, including Kenneth Co, Humza Iqbal, and Vincent Tjeng. We are also grateful to Daniel Filan, Humza Iqbal, Stefan Sietzen, and Vincent Tjeng for remarks on a draft.
    </p>
 -->
    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <d-bibliography src="bibliography.bib"></d-bibliography>

</body>
